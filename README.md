# Projet : Multiplication Matrice-Vecteur ParallÃ¨le
## MPI + OpenMP en C++

---

## ğŸ“‹ Description du projet

ImplÃ©mentation parallÃ¨le de la multiplication matrice-vecteur avec trois approches :
1. **SÃ©quentielle** : Version de rÃ©fÃ©rence
2. **MPI** : ParallÃ©lisation entre processus
3. **Hybride MPI + OpenMP** : ParallÃ©lisation Ã  deux niveaux

### Objectif
Comparer les performances des diffÃ©rentes stratÃ©gies de parallÃ©lisation et analyser le speedup et l'efficacitÃ©.

---

## ğŸ“ Structure du projet

```
projet/
â”œâ”€â”€ matmul_sequential.cpp    # Version sÃ©quentielle
â”œâ”€â”€ matmul_mpi.cpp            # Version MPI
â”œâ”€â”€ matmul_hybrid.cpp         # Version hybride MPI + OpenMP
â”œâ”€â”€ compile_hybrid.bat        # Script de compilation (hybride)
â”œâ”€â”€ compile_mpi.bat           # Script de compilation (MPI)
â”œâ”€â”€ benchmark.bat             # Script de benchmark automatique
â”œâ”€â”€ collect_results.bat       # Script de collecte de rÃ©sultats
â”œâ”€â”€ analyze_performance.py    # Script Python pour graphiques
â””â”€â”€ README.md                 # Ce fichier
```

---

## ğŸ”§ Compilation

### PrÃ©requis
- **Compilateur** : g++ (MinGW sur Windows)
- **MPI** : Microsoft MPI (MS-MPI)
- **OpenMP** : Inclus avec g++

### Compiler les programmes

```bash
# Version sÃ©quentielle
g++ -O3 -Wall -Wextra -std=c++17 -o matmul_sequential.exe matmul_sequential.cpp

# Version MPI
./compile_mpi.bat

# Version hybride (MPI + OpenMP)
./compile_hybrid.bat
```

---

## ğŸš€ ExÃ©cution

### Version sÃ©quentielle
```bash
./matmul_sequential.exe <rows> <cols>
# Exemple:
./matmul_sequential.exe 1000 1000
```

### Version MPI
```bash
mpiexec -n <num_processes> matmul_mpi.exe <rows> <cols>
# Exemples:
mpiexec -n 2 matmul_mpi.exe 1000 1000
mpiexec -n 4 matmul_mpi.exe 2000 2000
```

### Version hybride (MPI + OpenMP)
```bash
mpiexec -n <num_proc> -env OMP_NUM_THREADS <num_threads> matmul_hybrid.exe <rows> <cols>
# Exemples:
mpiexec -n 2 -env OMP_NUM_THREADS 2 matmul_hybrid.exe 1000 1000
mpiexec -n 2 -env OMP_NUM_THREADS 4 matmul_hybrid.exe 2000 2000
mpiexec -n 4 -env OMP_NUM_THREADS 2 matmul_hybrid.exe 5000 5000
```

---

## ğŸ“Š Benchmarks

### Lancer tous les tests automatiquement
```bash
./benchmark.bat
```

Ce script teste 7 configurations diffÃ©rentes avec une matrice 2000Ã—2000.

### Collecter des rÃ©sultats pour plusieurs tailles
```bash
./collect_results.bat
```

Teste les tailles : 500Ã—500, 1000Ã—1000, 2000Ã—2000, 5000Ã—5000

---

## ğŸ“ˆ Analyse des rÃ©sultats

### GÃ©nÃ©rer les graphiques
```bash
python analyze_performance.py
```

GÃ©nÃ¨re deux graphiques :
- `performance_analysis.png` : Comparaison des 7 configurations
- `scalability_analysis.png` : ScalabilitÃ© selon la taille

---

## ğŸ¯ RÃ©sultats obtenus (Matrice 2000Ã—2000)

| Configuration | Temps (ms) | GFLOPS | Speedup | EfficacitÃ© |
|--------------|------------|--------|---------|------------|
| SÃ©quentiel   | 4.322      | 1.851  | 1.00x   | 100%       |
| MPI 1 proc   | 4.336      | 1.845  | 1.00x   | 100%       |
| **MPI 2 proc**   | **2.545**      | **3.144**  | **1.70x**   | **85%**        |
| **MPI 4 proc**   | **2.230**      | **3.587**  | **1.94x**   | **48%**        |
| Hybrid 2Ã—2   | 4.102      | 1.950  | 1.05x   | 26%        |
| Hybrid 2Ã—4   | 2.785      | 2.873  | 1.55x   | 19%        |
| Hybrid 4Ã—2   | 3.547      | 2.256  | 1.22x   | 15%        |

### ğŸ† Meilleure configuration : **MPI 4 processus**
- Speedup : **1.94x**
- Performance : **3.587 GFLOPS**

---

## ğŸ’¡ Observations

### Points positifs âœ…
1. **MPI pur** donne les meilleures performances jusqu'Ã  4 processus
2. Speedup quasi-linÃ©aire avec 2 processus (85% d'efficacitÃ©)
3. Bon speedup avec 4 processus (48% d'efficacitÃ©)
4. VÃ©rification systÃ©matique de la correction des rÃ©sultats

### Limitations âš ï¸
1. Version hybride moins performante sur matrices moyennes (overhead OpenMP)
2. ScalabilitÃ© limitÃ©e au-delÃ  de 4 processus (overhead communication)
3. EfficacitÃ© diminue avec le nombre de workers

### Pourquoi MPI > Hybride ici ?
- Matrice 2000Ã—2000 trop petite pour justifier l'overhead OpenMP
- CrÃ©ation/synchronisation de threads coÃ»teuse
- Communication MPI suffit pour cette taille

---

## ğŸ”¬ Pistes d'amÃ©lioration

1. **Tester avec des matrices plus grandes** (10000Ã—10000, 20000Ã—20000)
   - L'approche hybride devrait mieux performer
   
2. **Optimisations algorithmiques**
   - Cache blocking / tiling
   - Vectorisation SIMD explicite (AVX/AVX2)
   - Loop unrolling
   
3. **Optimisations MPI**
   - Recouvrement calcul/communication (MPI_Isend/Irecv)
   - RÃ©duction du nombre de communications
   
4. **Optimisations OpenMP**
   - Ajuster le scheduling (static, dynamic, guided)
   - Affinity des threads
   - NUMA awareness

---

## ğŸ“ MÃ©thodologie

### DÃ©composition MPI
- Distribution des **lignes** de la matrice entre processus
- **Broadcast** du vecteur Ã  tous les processus
- Calcul **local** sur chaque processus
- **Gather** des rÃ©sultats sur le processus 0

### ParallÃ©lisation OpenMP
- ParallÃ©lisation de la **boucle externe** (lignes)
- Vectorisation de la **boucle interne** (produit scalaire)
- RÃ©duction pour Ã©viter les race conditions

### VÃ©rification
- Recalcul du premier Ã©lÃ©ment du rÃ©sultat
- Comparaison avec une tolÃ©rance de 10â»â¶

---

## ğŸ‘¥ Auteur
Zeineb LOUATI - M2 IA

## ğŸ“… Date
Janvier 2025

---

## ğŸ“š RÃ©fÃ©rences
- Documentation OpenMP : https://www.openmp.org/
- Documentation MPI : https://www.mpich.org/documentation/
- Microsoft MPI : https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi
